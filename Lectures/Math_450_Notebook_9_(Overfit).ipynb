{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 9 (Overfit).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNL8cK7+2C3oaypCv24O/K2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Lectures/Math_450_Notebook_9_(Overfit).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTQQSTcZvV0p"
      },
      "source": [
        "# Coding lecture 9 of Math 450\n",
        "\n",
        "Things need to know:\n",
        "\n",
        "## Python\n",
        "- Dictionary\n",
        "- Generator, iterator, `iter()`, `next()`, `enumerate()`, `try: except:` flow control.\n",
        "- Matrix-vector multiplications and \"broadcastability\".\n",
        "\n",
        "\n",
        "## PyTorch\n",
        "- `loss.backward()` vs hand computation.\n",
        "- Why `with torch.no_grad():` is necessary in manual gradient descent computation and inference.\n",
        "- Build simple neural network using `torch.nn.Sequential()`\n",
        "- Build complex neural network like CNN using `torch.nn.Module` class, `constructor`, inheritance, the usage of `super`.\n",
        "- Hand-compute gradient descent for a binary classification problem using torch `DataLoader` interface for (mini-batch) SGD.\n",
        "- How to implement autograd using `Optimizer` class.\n",
        "- Convolution neural network (CNN, to be learned)\n",
        "\n",
        "## Project\n",
        "- Implement an optimizer to train a CNN to classify handwritten Japanese characters.\n",
        "- Tune it using validation to achieve reasonabe accuracy.\n",
        "\n",
        "\n",
        "## Today\n",
        "- Overfit and validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrG4kRv7wFTn"
      },
      "source": [
        "## Data fitting\n",
        "\n",
        "For a set of data $\\{(x^{(i)},y^{(i)})\\}_{i=1}^{N}$, the NN model only \"fits\" the data roughly, not precisely. Yet we can achieve reasonably well accuracy with it.\n",
        "\n",
        "## Types of data fitting\n",
        "\n",
        "- **Interpolation**: suppose we know $n+1$ distinct grid points\n",
        "$x_0, x_1, x_2, \\dots, x_n$, and the values the values at each of these\n",
        "points as $f_k = f(x_k)$, but we have no idea of what $f$'s analytical expression is. Then the problem of interpolation is to find an approximation of $h(x)$ that is defined at any point $x \\in [a, b]$ that **coincides** with $f(x)$ at $x_k$.\n",
        "\n",
        "- **Regression**: we can also consider a regression model, to minimize the mean square error $\\dfrac{1}{n}\\|f(x_k) - h(x_k; W)\\|^2$, where $h(x_k; W)$ is the NN's output.\n",
        "\n",
        "\n",
        "## Tools to use\n",
        "Today we will borrow something from `scikit-learn` package.\n",
        "\n",
        "Reference: Adapted from [https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html) to be more readable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-pOkF4KxzTi"
      },
      "source": [
        "## Data\n",
        "\n",
        "`X_train`, `y_train` are our training data. In the following example, we have 10 of them.\n",
        "\n",
        "## Model\n",
        "We will use the linear regression in `scikit-learn `package to fit not just a linear function but a polynomial function of any degree, e.g. $h(x) = w_{10} x^{10} + w_9 x^9 + \\dots + w_1 x + b$, to the data. \n",
        "\n",
        "Remark: for those of us who are interested, we are essentially using the Vandermonde matrix by adding $x^p$ as features. \n",
        "\n",
        "## Validation\n",
        "We choose a bunch of testing points, see if our model (built from only 10 noisy samples) approximates our true function $x^2$ to a reasonable accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3irzPsl2vPef"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from sklearn.pipeline import Pipeline # for easier fitting using high degree polynomials testing\n",
        "from sklearn.preprocessing import PolynomialFeatures # evaluating polynomials at points\n",
        "from sklearn.linear_model import LinearRegression # we have used this before"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnij2OrJyQnO"
      },
      "source": [
        "np.random.seed(42)\n",
        "X_train = np.linspace(0,2,10)\n",
        "# true function is x^2, adding some noise\n",
        "true_function = lambda x: x**2\n",
        "y_train = true_function(X_train) + np.random.normal(0,0.5, size=10)\n",
        "plt.scatter(X_train, y_train, s=40, alpha=0.8);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNZrqzKiySaN"
      },
      "source": [
        "# linear regression\n",
        "poly_degree = 1\n",
        "polynomial_features = PolynomialFeatures(degree=poly_degree, include_bias=True)\n",
        "linear_regression = LinearRegression()\n",
        "pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_FMi179yimi"
      },
      "source": [
        "pipeline.fit(X_train.reshape(-1,1), y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHNPCFnhyjep"
      },
      "source": [
        "num_samples = 100\n",
        "X_test = np.linspace(0, 2, num_samples) # this the the testing points\n",
        "y_pred = pipeline.predict(X_test.reshape(-1,1)) \n",
        "y_true = true_function(X_test)\n",
        "error = np.mean((y_pred - y_true)**2)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(X_test, y_pred, linewidth = 2, label=\"Model's prediction\")\n",
        "plt.plot(X_test, y_true, linewidth = 2, label=\"True function\")\n",
        "plt.scatter(X_train, y_train, edgecolor='b', s=40, label=\"Training samples\")\n",
        "plt.legend(loc='best', fontsize = 'x-large')\n",
        "plt.title(f\"Mean Square Error = {error:.2e}\", fontsize = 'xx-large')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G2R4px6y9Vx"
      },
      "source": [
        "# What if we increase the degree?\n",
        "Try increasing the degree gradually in `PolynomialFeatures()` (since we have packed `PolynomialFeatures()` and `LinearRegression()` into one class, we can use pipeline). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSSxL1PeyzDx"
      },
      "source": [
        "# now we use pipeline to change the polynomial_features directly w/o redefine it\n",
        "# better than the scikit-learn's example's clumsy usage of pipeline\n",
        "pipeline.set_params(polynomial_features__degree=3)\n",
        "pipeline.fit(X_train.reshape(-1,1), y_train)\n",
        "\n",
        "## validation\n",
        "num_samples = 100\n",
        "X_test = np.linspace(0, 2, num_samples) # this the the testing points\n",
        "y_pred = pipeline.predict(X_test.reshape(-1,1)) # this the value predicted by the model\n",
        "y_true = true_function(X_test)\n",
        "error = np.mean((y_pred - y_true)**2)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(X_test, y_pred, linewidth = 2, label=\"Model's prediction\")\n",
        "plt.plot(X_test, y_true, linewidth = 2, label=\"True function\")\n",
        "plt.scatter(X_train, y_train, edgecolor='b', s=40, label=\"Training samples\")\n",
        "plt.legend(loc='best', fontsize = 'x-large')\n",
        "plt.title(f\"Mean Square Error = {error:.2e}\", fontsize = 'xx-large');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HmR8hNQ0LqJ"
      },
      "source": [
        "# Metric for regression problem\n",
        "\n",
        "Coefficient of determination $R^2$\n",
        "$$\n",
        "R^2\\Big(\\mathbf{y}^{\\text{Actual}}, \\mathbf{y}^{\\text{Pred}}\\Big) = 1 - \\frac{\\displaystyle\\sum_{i=1}^{n_{\\text{test}}} \\left(y^{(i),\\text{Actual}} - y^{(i),\\text{Pred}}\\right)^2}{\\displaystyle\\sum_{i=1}^{n_\\text{test}} (y^{(i),\\text{Actual}} - \\bar{y}^{\\text{Actual}})^2}\n",
        "\\quad \n",
        "\\text{ where }\\; \\bar{y}^{\\text{Actual}} = \\displaystyle\\frac{1}{n_{\\text{test}}} \n",
        "\\sum_{i=1}^{n_\\text{test}} y^{(i),\\text{Actual}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXyEeY0fzLQk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}