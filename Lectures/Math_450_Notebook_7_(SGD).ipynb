{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 7 (SGD).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbsKAPp4wUxDNFjnuPJ2LF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Lectures/Math_450_Notebook_7_(SGD).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbmEhyaK7bh1"
      },
      "source": [
        "# Coding lecture 7 of Math 450\n",
        "\n",
        "## Last two weeks\n",
        "\n",
        "- Explore MNIST dataset.\n",
        "- Generator, iterator, `iter()`, `next()`, `enumerate()`, `try: except:` flow control.\n",
        "- Matrix-vector multiplications and \"broadcastability\".\n",
        "- `loss.backward()` vs hand computation.\n",
        "- Why `with torch.no_grad():` is necessary.\n",
        "- Build simple neural network using `torch.nn.Sequential()`\n",
        "- Gradient descent for a binary classification problem.\n",
        "\n",
        "\n",
        "## Today\n",
        "- Class and object-oriented programming primer. `constructor`, inheritance, `super`.\n",
        "- Torch `DataLoader` interface for (mini-batch) SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKE1UBwe_uen"
      },
      "source": [
        "# Stochastic Gradient Descent\n",
        "\n",
        "Suppose our loss function is still:\n",
        "\n",
        "$$f := f(\\mathbf{w}; X,\\mathbf{y}) =  \\frac{1}{N}\\sum_{i=1}^N f(\\mathbf{w}; \\mathbf{x}^{(i)},y^{(i)}),$$\n",
        "\n",
        "where $X = (\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)})^{\\top}$ are the training samples, $\\mathbf{y} = (y^{(1)}, \\dots, y^{(N)})^{\\top}$ are the labels/taget values for the training samples.\n",
        "\n",
        "> Choose initial guess $\\mathbf{w}_0$, step size (learning rate) $\\alpha$, number of inner iterations $N$, number of epochs $n_E$ <br><br>\n",
        ">    Set $\\mathbf{w}_{N+1} = \\mathbf{w}_0$ for epoch $e=0$<br>\n",
        ">    For epoch $e=1,2, \\cdots, n_E$<br>\n",
        ">    &nbsp;&nbsp;&nbsp;&nbsp; $\\mathbf{w}_{0}$ for the current epoch is $\\mathbf{w}_{N+1}$ for the previous epoch.<br>\n",
        ">    &nbsp;&nbsp;&nbsp;&nbsp; Randomly shuffle the samples so that $\\{\\mathbf{x}^{(m)},y^{(m)}\\}_{m=1}^N$ is a permutation of the original dataset.<br>\n",
        ">    &nbsp;&nbsp;&nbsp;&nbsp; For $m=0,1,2, \\cdots, M$<br>\n",
        ">    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{m+1} = \\mathbf{w}_m - \\alpha \\nabla f_i(\\mathbf{w}; \\mathbf{x}^{(m)},y^{(m)})$\n",
        "\n",
        "One outer iteration is called a completed *epoch* (sweeping all samples once).\n",
        "\n",
        "### Remark\n",
        "This is the vanilla SGD: Single gradient evaluation at each iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH916-TZGa5S"
      },
      "source": [
        "# (Practice) Mini-batch SGD\n",
        "\n",
        "In the vanilla SGD, each parameter $\\mathbf{w}$ update is computed w.r.t one training sample randomly selected. In mini-batch SGD, the update is computed for a mini-batch (a small number of training samples), as opposed to a single example. The reason for this is twofold: \n",
        "* This reduces the variance in the parameter update and can lead to more stable convergence.\n",
        "* This allows the computation to be more efficient (less overhead), since the training code is already written in a vectorized way. \n",
        "\n",
        "A typical mini-batch size is $2^k$ (32, 256, etc), although the optimal size of the mini-batch can vary for different applications, and size of dataset (e.g., AlphaGo training uses mini-batch size of 2048 board images).\n",
        "\n",
        "> Choose initial guess $\\mathbf{w}_0$, learning rate $\\alpha$, <br>\n",
        "batch size $b$, number of inner iterations $M= \\lfloor N/n_B \\rfloor$, number of epochs $n_E$ <br><br>\n",
        ">    Set $\\mathbf{w}_{M+1} = \\mathbf{w}_0$ for epoch $e=0$<br>\n",
        ">    For epoch $e=1,2, \\cdots, n_E$<br>\n",
        ">    &nbsp;&nbsp;&nbsp;&nbsp; $\\mathbf{w}_{0}$ for the current epoch is $\\mathbf{w}_{M+1}$ for the previous epoch.<br>\n",
        ">    &nbsp;&nbsp;&nbsp;&nbsp; Randomly shuffle the training samples.<br>\n",
        ">    &nbsp;&nbsp;&nbsp;&nbsp; For $m=0,1,2, \\cdots, M$<br>\n",
        ">    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{m+1} = \\mathbf{w}_m -  \\frac{\\alpha}{n_B}\\sum_{i=1}^{n_B} \\nabla f(\\mathbf{w}; \\mathbf{x}^{(bm+i)},y^{(bm+i)})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhy_R_tu7Uyf"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RXA8wrMBf3E"
      },
      "source": [
        "!wget https://sites.wustl.edu/scao/files/2021/03/MNIST.tar_.gz\n",
        "!mv MNIST.tar_.gz MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztr4fOIGIKHQ"
      },
      "source": [
        "train = datasets.MNIST(root='./', \n",
        "                       train=True, \n",
        "                       download=True, \n",
        "                       transform = transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i-VjA_aILPT"
      },
      "source": [
        " train_loader = DataLoader(train, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5JsXHTnIz7S"
      },
      "source": [
        "# how to convert this into a generator?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XygY0E_oJQUz"
      },
      "source": [
        "## Network\n",
        "How to implement\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(784, 128), # 784 = 28*28\n",
        "            nn.ReLU(), # activation\n",
        "            nn.Linear(128, 32), # 2nd hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 10) # output layer\n",
        "        )\n",
        "```\n",
        "using the `torch.nn` neural network class template."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MFGJuJcJWF-"
      },
      "source": [
        "# class implementation which we will cover in next class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        # super is a keyword for \n",
        "        # constructor inheritance\n",
        "        # now we have a template of nn.Module interface\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 10)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # train data (-1, 28, 28) --> (-1, 28*28)\n",
        "        # in the implementation above\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUYv-SX9KvS8"
      },
      "source": [
        "# go through an example of a simple class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMFBJDgbKCYq"
      },
      "source": [
        "model = MLP()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJuQtxNIKypA"
      },
      "source": [
        "# How to train using this interface?\n",
        "\n",
        "- Train loader iterator.\n",
        "- Model.\n",
        "- Loss function!\n",
        "- Optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyHtPUcMLGc-"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "numEpochs = 10\n",
        "learning_rate = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv1B9odXK0Ps"
      },
      "source": [
        "for i, epoch in enumerate(range(numEpochs)):\n",
        "     \n",
        "  # arrays for checking accuracy\n",
        "  target_all = []\n",
        "  output_all = []\n",
        "\n",
        "  for data, target in train_loader:\n",
        "    # data, target = data.to(device), target.to(device)\n",
        "    \n",
        "    # model prediction\n",
        "    output = model(data)\n",
        "\n",
        "    # loss function\n",
        "    loss = loss_func(output, target)\n",
        "\n",
        "    # for later accuracy checking use\n",
        "    target_all.append(target.detach().numpy())\n",
        "    output_all.append(output.detach().numpy())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "    \n",
        "    # autograd to do backprop\n",
        "    loss.backward()\n",
        "\n",
        "    # GD\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "    \n",
        "      # accuracy after 1 epoch\n",
        "  target_all = np.array(target_all)\n",
        "  output_all = np.array(output_all)\n",
        "  acc = (target_all == output_all).float().mean()\n",
        "  print(f\"\\n Epoch {i+1} accuracy: {100*acc:.2f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TEtFPJ_NT5S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}