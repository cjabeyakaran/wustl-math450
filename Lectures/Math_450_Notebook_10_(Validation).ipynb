{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 10 (Validation).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGU1+FxSoKJa/Mp2J1gzAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Lectures/Math_450_Notebook_10_(Validation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWgIuaGfIBj2"
      },
      "source": [
        "# Coding lecture 10 of Math 450\n",
        "\n",
        "## Last couple of weeks\n",
        "- A complete pipeline of training a machine learning model\n",
        "\n",
        "## Today\n",
        "- How to build a bigger and more complex neural network.\n",
        "- Set up a validation strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9wNb-1mJ94C"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import Optimizer\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPSmTtOBKGiu"
      },
      "source": [
        "train = datasets.MNIST(root='./', \n",
        "                       train=True, \n",
        "                       download=True, \n",
        "                       transform = transforms.ToTensor());\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=8) \n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_size: int = 28*28,\n",
        "                 output_size: int = 10):\n",
        "        super(MLP, self).__init__() \n",
        "        self.linear0 = nn.Linear(input_size, 256)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(256, output_size)\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = x.view(x.size(0), -1) \n",
        "        x1 = self.linear0(x)\n",
        "        a1 = self.activation(x1)\n",
        "        output = self.linear1(a1)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikMGFZJgKYYP"
      },
      "source": [
        "\n",
        "class SGD(Optimizer): # subclass of Optimizer\n",
        "    \"\"\"\n",
        "    Implements the vanilla SGD simplified \n",
        "    from the torch official one for Math 450 WashU\n",
        "    \n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate\n",
        "        \n",
        "    Example:\n",
        "        >>> optimizer = SGD(model.parameters(), lr=1e-2)\n",
        "        >>> optimizer.zero_grad()\n",
        "        >>> loss_fn(model(input), target).backward()\n",
        "        >>> optimizer.step()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, # params: model.parameters()\n",
        "                       lr: float = 1e-3, # input: type = value\n",
        "                 ): \n",
        "        defaults = dict(lr=lr) \n",
        "        # add a default attribute that can be accessed\n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None): \n",
        "      '''\n",
        "      step(): w_{k+1} = w_k - alpha*grad f(w_k)\n",
        "      '''  \n",
        "      for group in self.param_groups:\n",
        "          for param in group['params']:\n",
        "              if param.grad is None:\n",
        "                  continue\n",
        "              grad_param = param.grad.data\n",
        "              \n",
        "              param.data = param.data - group['lr']*grad_param\n",
        "      return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3j055o9KtP2"
      },
      "source": [
        "model = MLP()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "epochs = 2\n",
        "learning_rate = 1e-3\n",
        "optimizer = SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6mqHyFIL24D"
      },
      "source": [
        "# How to build a bigger net?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jX8yoiNK9ZG"
      },
      "source": [
        "# pipeline\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    model.train() # formalism, useful when we have dropout\n",
        "    \n",
        "    loss_vals = []\n",
        "    \n",
        "    with tqdm(total=len(train_loader)) as pbar: # progress bar\n",
        "      for data, targets in train_loader:\n",
        "          \n",
        "        # forward pass\n",
        "        outputs = model(data)\n",
        "        \n",
        "        # loss function\n",
        "        loss = loss_func(outputs, targets)\n",
        "        \n",
        "        # record loss function values .item()\n",
        "        loss_vals.append(loss.item())\n",
        "        \n",
        "        # clean the gradient from last iteration\n",
        "        # param.grad is not zero in last iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backprop\n",
        "        # autograd\n",
        "        loss.backward()\n",
        "        \n",
        "        # stochastic gradient descent\n",
        "        # no with torch.no_grad(): block, param operation is using .data\n",
        "        optimizer.step()\n",
        "        \n",
        "        # check accuracy\n",
        "\n",
        "        # tqdm template\n",
        "        desc = f\"epoch: [{epoch+1}/{epochs}] loss: {np.mean(loss_vals):.4f}\"\n",
        "        pbar.set_description(desc)\n",
        "        pbar.update()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQlQFtslL9Gm"
      },
      "source": [
        "# How to validate?\n",
        "\n",
        "In order to make an informed choice, we need a way to *validate* that our model and our hyperparameters are a good fit to the data.\n",
        "While this may sound simple, there are some pitfalls that you must avoid to do this effectively.\n",
        "\n",
        "\n",
        "Model validation is very simple: making use of \"holdout\" validation sets and cross-validation for more robust model evaluation. We hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance. \n",
        "This splitting can be done using the ``train_test_split`` utility in Scikit-Learn:\n",
        "\n",
        "## Reference:\n",
        "- Python data science handbook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGFev_UNL84b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMVlM4y4M-x8"
      },
      "source": [
        "X = train.data\n",
        "y = train.targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pkyI24fNCnU"
      },
      "source": [
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, random_state=0, train_size=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HxCd_QpNWj5"
      },
      "source": [
        "train_set = TensorDataset(X_tr, y_tr)\n",
        "train_loader = DataLoader(train_set, batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6mkU5dlNjum"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}