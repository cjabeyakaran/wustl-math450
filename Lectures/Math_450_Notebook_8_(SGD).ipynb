{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 8 (SGD).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpHqH5ozfrpXwUVNJszsmi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Lectures/Math_450_Notebook_8_(SGD).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJZ07UQUTOCO"
      },
      "source": [
        "# Coding lecture 8 of Math 450\n",
        "\n",
        "## Last three weeks\n",
        "\n",
        "- MNIST\n",
        "- Generator, iterator, `iter()`, `next()`, `enumerate()`, `try: except:` flow control.\n",
        "- Matrix-vector multiplications and \"broadcastability\".\n",
        "- `loss.backward()` vs hand computation.\n",
        "- Why `with torch.no_grad():` is necessary in manual gradient descent computation.\n",
        "- Build simple neural network using `torch.nn.Sequential()`\n",
        "- Gradient descent for a binary classification problem.\n",
        "- Torch `DataLoader` interface for (mini-batch) SGD.\n",
        "\n",
        "# Today\n",
        "- More on class and object-oriented programming. `constructor`, inheritance, the usage of `super`.\n",
        "- PyTorch SGD training complete pipeline template.\n",
        "- A new type: dictionary `dict`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8gjAVrkYH8O"
      },
      "source": [
        "# A complete pipeline\n",
        "\n",
        "- Data preparation\n",
        "- Train-Validation split (will be covered later)\n",
        "- Model\n",
        "- Choose an optimizer or write one on our own.\n",
        "- Choose an scheduler or write one on our own (optional, will be covered later).\n",
        "- Choose the proper loss function.\n",
        "- Train!\n",
        "- Inference (for our final project, will be covered later)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b7eqCbVTMPz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge-qWZr48Clu"
      },
      "source": [
        "!wget https://sites.wustl.edu/scao/files/2021/03/MNIST.tar_.gz \n",
        "!mv MNIST.tar_.gz MNIST.tar.gz \n",
        "!tar -zxvf MNIST.tar.gz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaYq1GNd7Vn1"
      },
      "source": [
        "train = datasets.MNIST(root='./', \n",
        "                       train=True, \n",
        "                       download=True, \n",
        "                       transform = transforms.ToTensor());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkqKRzDi7Y26"
      },
      "source": [
        "train_loader = DataLoader(train, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJpd2ka3ABzX"
      },
      "source": [
        "class MLP(nn.Module): \n",
        "    def __init__(self, \n",
        "                 input_size: int):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear0 = nn.Linear(input_size, 256)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(256, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1) \n",
        "        x1 = self.linear0(x)\n",
        "        a1 = self.activation(x1)\n",
        "        output = self.linear1(a1)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlL90yAcAUAG"
      },
      "source": [
        "model = MLP(input_size=28*28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBIRXf7nAssJ"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "In this class we will learn how to write an optimizer.\n",
        "\n",
        "#### Reference: \n",
        "Final project start code: https://www.kaggle.com/scaomath/washu-math-450-sp21-final-project-starter#Final-project:-write-our-own-optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDycf51BAja_"
      },
      "source": [
        "from torch.optim import Optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5riw-HrA_9Q"
      },
      "source": [
        "class SGD(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the vanilla SGD simplified from the torch official one\n",
        "    for Math 450 WashU\n",
        "    \n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate\n",
        "        \n",
        "    Example:\n",
        "        >>> optimizer = SGD(model.parameters(), lr=1e-2)\n",
        "        >>> optimizer.zero_grad()\n",
        "        >>> loss_fn(model(input), target).backward()\n",
        "        >>> optimizer.step()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, \n",
        "                       lr: float = 1e-3):\n",
        "        defaults = dict(lr=lr) # add a default attribute that can be accessed\n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None): \n",
        "      # we can ignore closure for now, useful in quasi-Newton\n",
        "        \n",
        "      for group in self.param_groups: # fixed in template\n",
        "\n",
        "          for param in group['params']:\n",
        "              if param.grad is None:\n",
        "                  continue\n",
        "              grad_param = param.grad.data\n",
        "\n",
        "              param.data -= group['lr']*grad_param\n",
        "\n",
        "      return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dRmHOKkB-37"
      },
      "source": [
        "# What is a dictionary?\n",
        "\n",
        "- key, value, item.\n",
        "- Two ways of initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PGciaDuAb3e"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL9WbE3mCK24"
      },
      "source": [
        "optimizer = SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7oA-76xCjKs"
      },
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    loss_vals = []\n",
        "    \n",
        "    with tqdm(total=len(train_loader)) as pbar:\n",
        "      for x, targets in train_loader:\n",
        "          \n",
        "        # forward pass\n",
        "        outputs = model(x)\n",
        "        \n",
        "        # loss function\n",
        "        loss = loss_func(outputs, targets)\n",
        "        \n",
        "        # record loss function values\n",
        "        loss_vals.append(loss.item())\n",
        "        \n",
        "        # clean the gradient from last iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backprop\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        optimizer.step()\n",
        "        \n",
        "        # check accuracy\n",
        "\n",
        "        # tqdm template\n",
        "        desc = f\"epoch: [{epoch+1}/{epochs}] loss: {np.mean(loss_vals):.4f}\"\n",
        "        pbar.set_description(desc)\n",
        "        pbar.update()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCoVZDNMCv9C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}