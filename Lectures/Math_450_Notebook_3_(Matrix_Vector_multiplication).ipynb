{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 3 (Matrix Vector multiplication).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP38Cg9qmqlfXlv75U1ddgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Math_450_Notebook_3_(Matrix_Vector_multiplication).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LzKCusA5bZE"
      },
      "source": [
        "# Coding Lecture 3 of Math 450\n",
        "\n",
        "Overall goal of the our class: make us learn machine learning in torch package.\n",
        "- Build our own neural net using Torch's LEGO-like blocks.\n",
        "- Write torch-like code from scratch.\n",
        "- Write our own optimizer.\n",
        "\n",
        "Today's goal:\n",
        "- Use matrix vector multiplication in `torch` to build a network.\n",
        "- `Dense` layer in `nn` module.\n",
        "\n",
        "This is a worksheet version of the notebook. We can follow along during the coding lecture and then download the annotated version in our Github repository.\n",
        "\n",
        "Download this notebook at: \n",
        "\n",
        "Reference: Numpy's neural network implementation from scratch: https://www.kaggle.com/scaomath/simple-mnist-numpy-from-scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soVvKDHb51Fk"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnc7a5-o7oM9"
      },
      "source": [
        "# Gradient of a single sample\n",
        "\n",
        "We have the following neural network.\n",
        "\n",
        "<img src=\"https://sites.wustl.edu/scao/files/2021/02/3Lnn.png\" alt=\"drawing\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy3ITSYm9iJF"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usumPTS06NWl"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# first go to Runtime->Runtime type->Select GPU as accelerator \n",
        "# then uncomment this to run on GPU\n",
        "# device = torch.device(\"cuda:0\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6wKnuuJ9UM1"
      },
      "source": [
        "# N is the sample size (or current mini-batch size); \n",
        "# D_in is input dimension;\n",
        "# N_H is hidden dimension; \n",
        "# D_out is output dimension.\n",
        "N, D_in, N_H, D_out = 1, 10, 5, 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgebif2c9Vbk",
        "outputId": "8889cf47-c315-454b-f4d6-14643f5a63bb"
      },
      "source": [
        "# Create random Tensors to hold input and outputs.\n",
        "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
        "# with respect to these Tensors during the backward pass.\n",
        "torch.manual_seed(42)\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229, -0.1863,  2.2082, -0.6380,\n",
            "          0.4617,  0.2674]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uysr0EEs9Wrv"
      },
      "source": [
        "# Create random Tensors for weights.\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "# because our data has zero mean, there is no need to include bias\n",
        "torch.manual_seed(42)\n",
        "w1 = torch.randn(D_in, N_H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(N_H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "# here the w1 and w2 are actually transposes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-ZnXIjJ-b0b"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\mathbf{z}^{(2)} &= W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} \\\\\n",
        "\\mathbf{a}^{(2)} &= f(\\mathbf{z}^{(2)}) \\\\\n",
        "\\mathbf{z}^{(3)} &= W^{(2)} \\mathbf{a}^{(2)} + \\mathbf{b}^{(2)} \\\\\n",
        "h(\\mathbf{x}; W, b) &= \\mathbf{a}^{(3)} = \\mathbf{z}^{(3)}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9d3rwy3-Y9v"
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI3Ui9NX-vfu"
      },
      "source": [
        "## Actual data in batch\n",
        "\n",
        "In the actual implementation, the data normaly comes in batch, i.e., a matrix. For example, input is a matrix $X \\in \\mathbb{R}^{N \\times d}$, $N$ is a number of samples in a batch, each row represents a sample $\\mathbf{x} \\in \\mathbb{R}^{1\\times d}$. The weight matrix $W$ is actually formulated as:\n",
        "$$\n",
        "W = \\left(\n",
        "\\begin{array}{cccc}| & | & | & | \\\\\n",
        "\\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_m \\\\\n",
        "| & | & | & |\n",
        "\\end{array}\\right),\n",
        "$$\n",
        "if the output dimension of the layer of interest is $m$. The vectorized formulation is, for example, from the input (layer 0, dimension $d$) to layer 1 (dimension $m$)\n",
        "$$\n",
        "A^{(1)} = X (W^{(0)})^{\\top} + B\n",
        "$$\n",
        "where $X \\in \\mathbb{R}^{N \\times d}$, $W^{(0)} \\in \\mathbb{R}^{m\\times d}$ (input from $d$ perceptrons, output from $m$ perceptrons), $B$ is a matrix with each row being the same $\\mathbf{b} \\in \\mathbb{R}^{1\\times m}$ (layer 1 has $m$ perceptrons and has $m$ biases if applicable).\n",
        "\n",
        "## Torch's nn module\n",
        "\n",
        "We will demo this batch-based operation using `torch`'s neural network module `nn`. `nn.Linear` applies an (affine) linear transformation to the incoming data:\n",
        "$$\n",
        "Y = X W^{\\top} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "Reference: https://pytorch.org/docs/stable/nn.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DIgmIGc_Rno"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71raJ2Fu_PGF"
      },
      "source": [
        "layer = nn.Linear(4, 3) # Wx+b transforms (4,1) vector to (3,1) vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np4MVu5J_QCi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkFMOc5FAMiW"
      },
      "source": [
        "## Gradient in Torch: autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFwWAu0kAL_3"
      },
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieBVxA66Aehx"
      },
      "source": [
        "Q = 3*a**3 - b**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzfJ0ypFApvm"
      },
      "source": [
        "L = Q.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oucQIwEXAqRA"
      },
      "source": [
        "L.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLFIjKn4BCPj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}