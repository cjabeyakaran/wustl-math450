{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 5 (GD).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+XiWRcTxGQ06XPzliug9Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Lectures/Math_450_Notebook_5_(GD).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxyfNfsXxw_d"
      },
      "source": [
        "# Coding lecture 4 of Math 450\n",
        "\n",
        "Overall goal of the our class: make us learn machine learning in torch package.\n",
        "- Build our own neural net using Torch's LEGO-like blocks.\n",
        "- Write torch-like code from scratch.\n",
        "- Write our own optimizer.\n",
        "\n",
        "## Last week\n",
        "\n",
        "- Explore MNIST dataset.\n",
        "- Generator, iterator, `iter()` and `next()`\n",
        "- `enumerate()`.\n",
        "- `with`.\n",
        "\n",
        "## Instructor's suggestions and tips\n",
        "- How to debug a `for` loop in a cell environment or a vectorized operation.\n",
        "- Good habit: keep a code repo of ourselves, a working copy of the HW, then a submit version. \n",
        "- Keep a list of typical solutions handy (to save some time Googling or trial and error of StackOverflow's answers). e.g., how to read a pickled file, how to get indices of entries from a list, etc.\n",
        "\n",
        "## Today's goals\n",
        "- f-string.\n",
        "- `try: except:` flow control.\n",
        "- More on matrix-vector multiplications (most important skill: dimension tracking). The terminology \"broadcastable\".\n",
        "- Backprop `loss.backward()` vs hand computation.\n",
        "- Gradient descent (manual implementation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiYrajEv2DFk"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhNInNab0zJ0"
      },
      "source": [
        "## f-string\n",
        "New in Python 3, very handy tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8bAqTGvwYX"
      },
      "source": [
        "# example of a timer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oqKwrrX1Aqw"
      },
      "source": [
        "# print the counter of iterations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBvxNjFsBwi-"
      },
      "source": [
        "## Try, except\n",
        "\n",
        "Often seen in many data loading routines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orv5D4xJB33_"
      },
      "source": [
        "# this line of code will give us error\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J45DaxNr1GUt"
      },
      "source": [
        "## Debugging `for` loops\n",
        "\n",
        "- How do we figure out what a `for` loop is doing?\n",
        "- From `for` loop to vectorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTc-q3ur1o4-"
      },
      "source": [
        "# load the mnist data\n",
        "\n",
        "train = datasets.MNIST('../data', train=True, download=True, transform = transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB5D_Mkx3QCq"
      },
      "source": [
        "idx = (train.targets==8)\n",
        "labels_new = train.targets[idx]\n",
        "data = train.train_data[idx]/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZm5dgeh3sdp",
        "outputId": "c76f7b75-9d77-4718-820c-514a6bf9cab6"
      },
      "source": [
        "n_data_all = len(data)\n",
        "idx = np.random.choice(range(n_data_all), size=10)\n",
        "data_new = data[idx].clone().detach()\n",
        "print(data_new.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "9EvgpmA84OZV",
        "outputId": "e35195ce-725f-4f68-c9cc-e497a2142107"
      },
      "source": [
        "i = 0\n",
        "plt.imshow(data_new[i,:,:], cmap='gray'); # plot the i-th sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAO2klEQVR4nO3db2iV9f/H8dfZqchc7MvmzrENWSxmkKgEWzAiR2c6CrccGxMkQw+LQZlDR5ibsGihmJWMvNPW7iiZKOJmbBVzgyy8s9mNpqSiN0wd21kcVJqWf9b1uxGdX6bnc+qc6/zRz/MBg+28vM7enXh5zeuz63w8juM4AvDAy0r3AABSg7IDlqDsgCUoO2AJyg5Y4qFUfrNffvlFP//8cyq/JWCVoqIi5efn3zt0EnD06FGnqqrKWbp0qdPV1RXzz4+OjjqS+OCDjyR9jI6ORu1f3D/Gz8zMqKOjQz09PRoYGFB/f7/OnTsX79MBSLK4yz42NqaioiLNmzdPjzzyiJYvX67h4WE3ZwPgorjLHgqFNHfu3MjXfr9foVDIlaEAuI+r8YAl4i673+/X5ORk5OtQKCS/3+/KUADcF3fZFy5cqPPnz+vixYu6efOmBgYGFAgE3JwNgIviXmd/6KGH1N7ertdff10zMzOqr69XSUmJm7MBcFFCv1RTUVGhiooKt2YBkERcoAMsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKUHbAEZQcsQdkBSyS0ZXMgENDs2bOVlZUlr9erQ4cOuTUXAJclVHZJ2r17t3Jzc92YBUAS8WM8YImEy97Y2Ki6ujrt37/fjXkAJElCP8bv27dPfr9f4XBYwWBQxcXFKisrc2s2AC5K6Mzu9/slSXl5eVq2bJnGxsZcGQqA++Iu+/Xr1zU9PR35/NixYyopKXFtMADuivvH+HA4rHXr1kmSZmZmVF1drSVLlrg2mE0efvhhY97c3GzM29vbo2aPP/648ViPx2PMHccx5uPj48b85ZdfjpqdOnXKeOzMzIwxx38Td9nnzZunL7/80s1ZACQRS2+AJSg7YAnKDliCsgOWoOyAJRK+EQaJq6+vN+Y7duww5levXo2anTt3znis1+s15k8++aQxLygoMOY//vhj1Oy9994zHrtt2zZjfvv2bWOOO3FmByxB2QFLUHbAEpQdsARlByxB2QFLUHbAEqyzp8ATTzxhzN9///2Env/jjz+Omm3dutV47KOPPmrMTbeoSn/e/Whi+m979913jceGQiFj3tXVZcxxJ87sgCUoO2AJyg5YgrIDlqDsgCUoO2AJyg5YgnX2FJiYmDDmFy5cMObFxcXGfM+ePf95pr/8/vvvxry3tzfu55akycnJqFlPT4/x2Pnz5yf0vWfNmhU1q6ioMB4baw+EXbt2xTVTOnFmByxB2QFLUHbAEpQdsARlByxB2QFLUHbAEqyzPwCCwWDUrKOjI4WT3O3AgQNRs4aGBuOxq1evNuYnT5405i0tLVGzZ555xnjsmjVrjPn9KOaZvbW1VeXl5aquro48duXKFQWDQVVVVSkYDBo3KQCQGWKWva6u7q7fdOru7lZ5ebkGBwdVXl6u7u7upA0IwB0xy15WVqacnJw7HhseHlZtba0kqba2VkNDQ8mZDoBr4rpAFw6H5fP5JEn5+fkKh8OuDgXAfQlfjfd4PPJ4PG7MAiCJ4ip7Xl6epqamJElTU1PKzc11dSgA7our7IFAQH19fZKkvr4+VVZWujoUAPfFXGdvaWnRyMiILl++rCVLlmj9+vVqamrShg0bdPDgQRUUFKizszMVsyKK/Pz8dI8Ql8HBQWP+0ksvGfNY98ObfP7558b84MGDcT93popZ9p07d97z8d27d7s+DIDk4ddlAUtQdsASlB2wBGUHLEHZAUtwi2sGiLbi8ZdYb3tcWloaNTO9nbIk/fbbb8Y8lqws8/nCtGXzO++8YzzWcRxj/s033xjzlStXRs1u3LhhPPb27dvG/H7EmR2wBGUHLEHZAUtQdsASlB2wBGUHLEHZAUuwzp4BBgYGjPmJEyeM+XPPPRc1M61zS9Lbb79tzJ966iljvnXrVmNuervo69evG481rZNL0tdff23McSfO7IAlKDtgCcoOWIKyA5ag7IAlKDtgCcoOWIJ19vtAf3+/MV+0aFHU7K233jIe++GHHxrzjz76yJi/8sorxnx6ejpq9tprrxmPZR3dXZzZAUtQdsASlB2wBGUHLEHZAUtQdsASlB2wBOvs94FPP/3UmDc1NUXN5syZYzz2+++/N+ax7mf/6aefjHkwGIyaHT9+3Hgs3BXzzN7a2qry8nJVV1dHHtu1a5deeOEFrVixQitWrNDRo0eTOiSAxMU8s9fV1Wn16tV37d6xdu1aNTY2Jm0wAO6KeWYvKytTTk5OKmYBkERxX6Dbu3evampq1NraqqtXr7o5E4AkiKvsq1at0pEjR3T48GH5fD5t377d7bkAuCyuss+ZM0der1dZWVlqaGiI+e6nANIvrrJPTU1FPh8aGlJJSYlrAwFIjphX41taWjQyMqLLly9ryZIlWr9+vUZGRnT69GlJUmFhoTo6OpI+qM3Gx8eNeU9PT9Rs8+bNxmNjraOb7keXpPb2dmPOWnrmiFn2nTt33vWY6Y3/AWQmfl0WsARlByxB2QFLUHbAEpQdsAS3uD4ATG/33NzcbDz2scceM+YXLlww5r29vcYcmYMzO2AJyg5YgrIDlqDsgCUoO2AJyg5YgrIDlmCd/QFQW1sbNZs1a1ZCz/30008b86qqKmM+ODiY0PeHezizA5ag7IAlKDtgCcoOWIKyA5ag7IAlKDtgCdbZHwCme9I9Hk9Cz+31eo15W1ubMWedPXNwZgcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKssz8Ali1bFjWbmZkxHtvf32/MV6xYYcyzsjhf3C9iln1iYkKbNm1SOByWx+PRypUrtWbNGl25ckUbN27U+Pi4CgsL1dnZqZycnFTMDCAOMf9a9nq92rx5s7766ivt379fX3zxhc6dO6fu7m6Vl5drcHBQ5eXl6u7uTsW8AOIUs+w+n08LFiyQJGVnZ6u4uFihUEjDw8ORt0Oqra3V0NBQcicFkJD/9A+uS5cu6dSpU1q8eLHC4bB8Pp8kKT8/X+FwOCkDAnDHvy77tWvX1NzcrLa2NmVnZ9+ReTyehG+4AJBc/6rst27dUnNzs2pqaiLvJpqXl6epqSlJ0tTUlHJzc5M3JYCExbwa7ziOtmzZouLiYgWDwcjjgUBAfX19ampqUl9fnyorK5M6KKJ79tlno2axlt7efPNNYx5r6Q33j5hl/+GHH3T48GHNnz8/8j++paVFTU1N2rBhgw4ePKiCggJ1dnYmfVgA8YtZ9tLSUp05c+ae2e7du10fCEBy8OtPgCUoO2AJyg5YgrIDlqDsgCW4xdVyRUVF6R4BKcKZHbAEZQcsQdkBS1B2wBKUHbAEZQcsQdkBS7DO/gDYs2dP1Ky1tdV4bG9vr9vjIENxZgcsQdkBS1B2wBKUHbAEZQcsQdkBS1B2wBKssz8AJicno2axdurx+/1uj4MMxZkdsARlByxB2QFLUHbAEpQdsARlByxB2QFLxFxnn5iY0KZNmxQOh+XxeLRy5UqtWbNGu3bt0oEDB5Sbmyvpz22cKyoqkj4w7tbT0xM1mzt3rvHYtrY2Yz4wMGDM29vbjTkyR8yye71ebd68WQsWLND09LTq6+v1/PPPS5LWrl2rxsbGpA8JIHExy+7z+eTz+SRJ2dnZKi4uVigUSvpgANz1n/7NfunSJZ06dUqLFy+WJO3du1c1NTVqbW3V1atXkzIgAHf867Jfu3ZNzc3NamtrU3Z2tlatWqUjR47o8OHD8vl82r59ezLnBJCgf1X2W7duqbm5WTU1NaqqqpIkzZkzR16vV1lZWWpoaNCJEyeSOiiAxMQsu+M42rJli4qLixUMBiOPT01NRT4fGhpSSUlJciYE4AqP4ziO6Q8cP35cr776qubPn6+srD//bmhpaVF/f79Onz4tSSosLFRHR0fkQp7pucrKylwaHcA/jY6OqrS09J5ZzKvxpaWlOnPmzF2Ps6YO3F/4DTrAEpQdsARlByxB2QFLUHbAEpQdsARlByxB2QFLUHbAEpQdsARlByxB2QFLUHbAEpQdsETM+9kBPBg4swOWoOyAJSg7YAnKDliCsgOWoOyAJSg7YImYbyWdDN999522bt2qP/74Qw0NDWpqakrHGPcUCAQ0e/ZsZWVlyev16tChQ2mbpbW1Vd9++63y8vLU398vSbpy5Yo2btyo8fFxFRYWqrOzUzk5ORkxW6Zs4x1tm/F0v3Zp3/7cSbHbt287lZWVzoULF5wbN244NTU1ztmzZ1M9RlQvvviiEw6H0z2G4ziOMzIy4pw8edJZvnx55LEPPvjA6erqchzHcbq6upwdO3ZkzGyffPKJ09PTk5Z5/i4UCjknT550HMdxfv31V6eqqso5e/Zs2l+7aHOl6nVL+Y/xY2NjKioq0rx58/TII49o+fLlGh4eTvUY94WysrK7zjzDw8Oqra2VJNXW1mpoaCgdo91ztkzh8/m0YMECSXduM57u1y7aXKmS8rKHQiHNnTs38rXf78+4/d4bGxtVV1en/fv3p3uUu4TD4cg2W/n5+QqHw2me6E6Zto3337cZz6TXLh3bn3OB7h/27dun3t5effbZZ9q7d69GR0fTPVJUHo9HHo8n3WNEZNo23v/cZvzv0vnapWv785SX3e/3a3JyMvJ1KBSS3+9P9RhR/TVLXl6eli1bprGxsTRPdKe8vLzIDrpTU1ORizqZIJO28b7XNuOZ8Nqlc/vzlJd94cKFOn/+vC5evKibN29qYGBAgUAg1WPc0/Xr1zU9PR35/NixYxm3FXUgEFBfX58kqa+vT5WVlWme6P9lyjbeTpRtxtP92kWbK1WvW1pucT169Ki2bdummZkZ1dfX64033kj1CPd08eJFrVu3TpI0MzOj6urqtM7W0tKikZERXb58WXl5eVq/fr2WLl2qDRs2aGJiQgUFBers7NT//ve/jJhtZGTkP2/jnQzRthlftGhRWl87N7c/jwf3swOW4AIdYAnKDliCsgOWoOyAJSg7YAnKDliCsgOW+D9V/RLfsRWsSAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI4ANbqW4Qz9"
      },
      "source": [
        "# problem 2 in HW, how to dissect this code\n",
        "W0 = torch.randn(28*28, 14*14)\n",
        "n_samples = len(data_new)\n",
        "X0 = data_new\n",
        "X1 = torch.zeros(n_samples, 14*14, 1)\n",
        "for i in range(n_samples):\n",
        "    sample = X0[i,:,:].view(-1,1)\n",
        "    X1[i,:] = W0.T.mm(sample)\n",
        "X2 = X1.view(-1,14,14)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWoCkSMz6i1i"
      },
      "source": [
        "# example of vectorized functions, mean, sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkst9xXhCN4l"
      },
      "source": [
        "## `torch.mm` and `*` in backprop\n",
        "\n",
        "\"broadcastable\": the tensor involved in an operation can be automatically expanded to be of equal sizes (without making copies of the data) to let the operation go through.\n",
        "\n",
        "General semantics\n",
        "Two tensors are “broadcastable” \n",
        "- Each tensor has at least one dimension.\n",
        "- When iterating over the dimension sizes, starting at the last dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt63AXfACVpq"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "n_samples = len(data_new)\n",
        "\n",
        "# X[i] is a row vector representing a sample\n",
        "X = data_new.view(-1, 28*28).clone().detach()\n",
        "\n",
        "# y[i] is a made-up label for i-th sample\n",
        "y = torch.randn((n_samples, 1), requires_grad=False)\n",
        "\n",
        "# W is the weight matrix transposed \n",
        "W = torch.randn((28*28, 1), requires_grad=True)\n",
        "L = 0.5*(X.mm(W) - y).square().mean()\n",
        "\n",
        "# backprop\n",
        "L.backward()\n",
        "\n",
        "# gradient\n",
        "with torch.no_grad():\n",
        "    gradW = W.grad"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBuSf27hDiv4"
      },
      "source": [
        "### Problem 4 in HW 4\n",
        "Implement\n",
        "$$ \n",
        "\\frac{\\partial L}{ \\partial (W^T)} = \\frac{1}{10} \\sum_{i=1}^{10} \n",
        "(W\\boldsymbol{x}^{(i)} -y^{(i)}) * \\boldsymbol{x}^{(i)}\n",
        "$$ \n",
        "\n",
        "First let us look at a simpler function of $\\boldsymbol{w}$\n",
        "$$\n",
        "f(z) = (z - y)^2\n",
        "$$\n",
        "and \n",
        "$$\n",
        "z = \\boldsymbol{w} \\cdot \\boldsymbol{x}\n",
        "$$\n",
        "then \n",
        "$$\n",
        "\\frac{\\partial f}{\\partial \\boldsymbol{w}} = 2(z - y) \\frac{\\partial z}{\\partial \\boldsymbol{w}} \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu9Dlw5QHnPV"
      },
      "source": [
        "# demo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTyfkomaE9nI"
      },
      "source": [
        "Consider multi-dimension\n",
        "$$\n",
        "f(z_1, z_2) = (z_1 - y_1)^2 + (z_2 - y_1)^2\n",
        "$$\n",
        "If we have 5 samples, each one has 3 features\n",
        "For $W^T\\in \\mathbb{R}^{3\\times 2 }$, $\\boldsymbol{x} \\in \\mathbb{R}^{5\\times 3} $\n",
        "$$\n",
        "\\boldsymbol{z} = \\boldsymbol{x} W^T \n",
        "$$\n",
        "then \n",
        "$$\n",
        "\\frac{\\partial f}{\\partial W^T} = 2(\\boldsymbol{z} - \\boldsymbol{y}) \\frac{\\partial z}{\\partial W^T} \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyJ_MYWLFA1d"
      },
      "source": [
        "## Full gradient descent code for 3-layer nn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Jb0d7XE81e"
      },
      "source": [
        "# N is the sample size (or current batch size); \n",
        "# D_in is input dimension;\n",
        "# N_H is hidden dimension; \n",
        "# D_out is output dimension.\n",
        "N, D_in, N_H, D_out = 64, 1000, 100, 3\n",
        "\n",
        "# Create random Tensors to hold input and outputs.\n",
        "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
        "# with respect to these Tensors during the backward pass.\n",
        "X = torch.randn(N, D_in, requires_grad=False)\n",
        "y = torch.randn(N, D_out, requires_grad=False)\n",
        "\n",
        "# Create random Tensors for weights.\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "w1 = torch.randn(D_in, N_H, requires_grad=True)\n",
        "w2 = torch.randn(N_H, D_out, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "for m in range(1000):\n",
        "    # Forward pass: compute predicted y using operations on Tensors; these\n",
        "    # are exactly the same operations we used to compute the forward pass using\n",
        "    # Tensors, but we do not need to keep references to intermediate values since\n",
        "    # we are not implementing the backward pass by hand.\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "    # Compute and print loss using operations on Tensors.\n",
        "    # Now loss is a Tensor of shape (1,)\n",
        "    # loss.item() gets the a scalar value held in the loss.\n",
        "    loss = (y_pred - y).pow(2).mean()\n",
        "    if m % 100 == 0:\n",
        "        print(f\"LS loss after {m}\", \n",
        "              f\"iterations is {loss.item()}\",)\n",
        "\n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
        "    # of the loss with respect to w1 and w2 respectively.\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    # because weights have requires_grad=True, but we don't need to track this\n",
        "    # in autograd.\n",
        "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
        "    # Recall that tensor.data gives a tensor that shares the storage with\n",
        "    # tensor, but doesn't track history.\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk1nnpN7DfOF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}