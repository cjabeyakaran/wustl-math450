{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Math 450 Notebook 6 (GD).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOygj0+nGBhbc3yt2bsEyRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Lectures/Math_450_Notebook_6_(GD).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kpgmh0G4rCF"
      },
      "source": [
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "### somehow torch.dataset malfunctioned after an update in March 2021\n",
        "### Facebook team issued a hotfix but apparently not loaded in the docker image\n",
        "### of Colab yet as of Mar 5, 2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYkmZChYx5AT"
      },
      "source": [
        "# Coding lecture 6 of Math 450\n",
        "\n",
        "## Last two weeks\n",
        "\n",
        "- Explore MNIST dataset.\n",
        "- Generator, iterator, `iter()`, `next()`, `enumerate()`, `try: except:` flow control.\n",
        "- Matrix-vector multiplications and \"broadcastability\".\n",
        "- `loss.backward()` vs hand computation.\n",
        "\n",
        "## Today\n",
        "- Why `with torch.no_grad():` is necessary.\n",
        "- Build simple neural network using `torch.nn.Sequential()`\n",
        "- Gradient descent for a binary classification problem.\n",
        "- (if time allows) Class and object-oriented programming primer. `constructor`, inheritance, `super`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIoX6q7B0OGe"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGu_fh4r1lT2"
      },
      "source": [
        "# MNIST\n",
        "- Load the data. Both train and validation (test) data.\n",
        "- Extract only 0, 1 labeled data.\n",
        "- Write a loss function compute the cross entropy.\n",
        "- Apply the gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIboolJM1kiI"
      },
      "source": [
        "# load the mnist data\n",
        "train = datasets.MNIST(root='./data', \n",
        "                       train=True, \n",
        "                       download=True, \n",
        "                       transform = transforms.ToTensor())\n",
        "valid = datasets.MNIST(root='./data', \n",
        "                       train=False, \n",
        "                       download=True, \n",
        "                       transform = transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mw_Phgh4_Di"
      },
      "source": [
        "idx_tr = (train.targets == 0) | (train.targets == 1) # getting 0 and 1 labeled data\n",
        "target_tr = train.targets[idx_tr]\n",
        "train_new = train.train_data[idx_tr].clone()/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0C-mgFv6ZoE"
      },
      "source": [
        "idx_valid = (valid.targets == 0) | (valid.targets == 1) # getting 0 and 1 labeled data\n",
        "target_val = valid.targets[idx_valid]\n",
        "valid_new = valid.test_data[idx_valid].clone()/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXuZDPZX6iCk"
      },
      "source": [
        "print(len(train_new), len(valid_new))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9TY8eB7aaS"
      },
      "source": [
        "fig, axes = plt.subplots(4,5, figsize=(12,10))\n",
        "axes = axes.flatten()\n",
        "indices = np.random.randint(0,len(train_new),size=20)\n",
        "for i, idx in enumerate(indices):\n",
        "    X = train_new[idx].clone().detach()/255\n",
        "    y = target_tr[idx]\n",
        "    axes[i].imshow(X, cmap='gray')\n",
        "    axes[i].axis('off') # hide the axes ticks\n",
        "    axes[i].set_title(str(int(y)), color= 'black', fontsize=25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRNdWjBb9pHM"
      },
      "source": [
        "# cross-entropy loss\n",
        "\n",
        "def cross_entropy_loss(yhat, y):\n",
        "    '''\n",
        "    Compute the cross entropy of yhat against y\n",
        "      - yhat: the sigmoid of the output of an NN\n",
        "      - y: 0 or 1, true target\n",
        "    '''\n",
        "    loss = - y * torch.log(yhat) - (1-y) * torch.log(1-yhat)\n",
        "    return loss.mean()\n",
        "\n",
        "def sigmoid(yhat):\n",
        "    return 1/(1 + torch.exp(-yhat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bzfi9Ij_77Q"
      },
      "source": [
        "# explain why the implementation above is naturally vectorized\n",
        "# randomly generate yhat\n",
        "\n",
        "# the dimension has to be consistent for yhat and y\n",
        "\n",
        "yhat = torch.randn((10,))\n",
        "yhat = sigmoid(yhat)\n",
        "y = target_tr[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pzxqsTIABvU"
      },
      "source": [
        "- y * torch.log(yhat) - (1-y) * torch.log(1-yhat) \n",
        "# 10 samples, a cross-entropy loss for each sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdkHDHgFAi7w"
      },
      "source": [
        "## this would yield bad results\n",
        "yhat = torch.randn((5,1))\n",
        "yhat = sigmoid(yhat)\n",
        "y = target_tr[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IcW0dH_BDSm"
      },
      "source": [
        "# `nn.Sequential()`\n",
        "\n",
        "- An NN container for LEGO'ing layers.\n",
        "- Good for beginners like us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBd1VuS4BC5I"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyuGKNbEBsc5"
      },
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StbUDx9_CIbj"
      },
      "source": [
        "# demonstrate summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BEg2KszCJZX"
      },
      "source": [
        "# class implementation which we will cover in next class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        # super is a keyword for \n",
        "        # constructor inheritance\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # train data (-1, 28, 28) --> (-1, 28*28)\n",
        "        # in the implementation above\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiAtUJREC_OO"
      },
      "source": [
        "# Gradient descent\n",
        "For $k=0,1,\\dots$, update $W_{k+1} = W_k - \\alpha \\nabla_W L$, where $W$ stands for the parameters of the NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H75yquLQDHyJ"
      },
      "source": [
        "# hyper-parameters, data preparation,  initialize the model\n",
        "\n",
        "numEpochs = 20 # number of epochs, 1 epoch means the model sweeps train data set once\n",
        "learning_rate = 1e-2 \n",
        "\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "X = train_new.view(train_new.size(0), -1)\n",
        "y = target_tr\n",
        "print(X.size(), y.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xH2P8-GDBVH"
      },
      "source": [
        "### cpu code\n",
        "\n",
        "for i, epoch in enumerate(range(numEpochs)):\n",
        "\n",
        "    y_hat = model(X)\n",
        "    y_hat = sigmoid(y_hat)\n",
        "\n",
        "    loss = cross_entropy_loss(y_hat, y)\n",
        "    \n",
        "    print(f\"cross entropy loss after {i}\", \n",
        "          f\"iterations is {loss.item()}\",)\n",
        "    \n",
        "    # accuracy\n",
        "    preds = (y_hat > 0.5).detach()\n",
        "    acc = (preds == y).float().mean()\n",
        "    print(f\"accuracy: {100*acc:.2f} \\n\")\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # autograd to do backprop\n",
        "    loss.backward()\n",
        "\n",
        "    # GD\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYcJaaSWHYHN"
      },
      "source": [
        "# preparation for GPU code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwtatVZwHeUo"
      },
      "source": [
        "# GPu training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c8UYp8bHxkX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}