{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "colab": {
      "name": "chw-4_YOUR_NAME.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaomath/wustl-math450/blob/main/Homework/chw_4_YOUR_NAME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Gyob8nJ8WJ"
      },
      "source": [
        "# Homework 4: A PyTorch pipeline\n",
        "Name:\n",
        "\n",
        "Wustlkey:\n",
        "\n",
        "Partner Name (if applicable):\n",
        "\n",
        "Partner Wustlkey (if applicable):\n",
        "\n",
        "### Submission instructions\n",
        "\n",
        "- Submit the modified python notebook as homework submission.\n",
        "- Group submission is enabled, you can submit this coding assignment with up to 1 teammate in our class. For instruction of how to do a group submission. Please refer to Canvas useful links.\n",
        "- You can google answers on StackOverflow, please attach the corresponding StackOverflow answer as comments. However, if the answer is converted to `torch` format, no credit will be awarded.\n",
        "- Do not change the number of cells! Please work in the cell provided. If we need extra cells for debugging and testing purposes, we can work at the end of this notebook, save everything as a backup for review, and delete the extra cells in the submitted version.\n",
        "\n",
        " \n",
        "\n",
        "### Instructions\n",
        "Do **not** use `for` loops to iterate along the number of feature dimension for computational purpose in any of our solutions! We are allowed to use `for` loops to display figures, iterating across train loader etc.\n",
        "Efficieny will be graded as well. For example if a problem asks us generate an array from 0 to 9: then\n",
        "```python\n",
        "x = []\n",
        "for i in range(10):\n",
        "    x.append(i)\n",
        "```\n",
        "this will only result a partial credit while\n",
        "```python\n",
        "x = np.arange(10)\n",
        "```\n",
        "or\n",
        "```python\n",
        "x = torch.arange(10)\n",
        "```\n",
        "will yield a full score.\n",
        "\n",
        "### Problems\n",
        "Below are 4 problems that helps us understand what a full pipeline in PyTorch looks like. Complete the coding tasks for credit. \n",
        "\n",
        "### Grading\n",
        "This homework has 4 problems, 5 points for each problem. The homework will be graded and the grade counts towards your course grade. \n",
        "\n",
        "## Coding environments and submission\n",
        "If we do not have `torch` installed on your computer, we have three ways to upload this notebook to [Google colab](https://colab.research.google.com/)：\n",
        "\n",
        "1. Open up Google Colab, choose `Upload` to upload this template and work there. After we have done working we can select `File->Download .ipynb`.\n",
        "2. Open up Google Colab, choose either `GitHub` or `Google Drive` to select the uploaded notebook in the corresponding website. After done working, we can sync the file to the corresponding GitHub or Google Drive copy.\n",
        "3. Use the \"Open in Colab\" button at the top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsaDULmwJ8WK"
      },
      "source": [
        "# import torch and numpy\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Optimizer\n",
        "# import torchvision \n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# import packages that help us plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76wfN0uOJ8WK"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "\n",
        "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
        "\n",
        "In the following cells, we will learn how to load and view this dataset for our toy models. \n",
        "\n",
        "Read more:[https://www.kaggle.com/c/digit-recognizer](https://www.kaggle.com/c/digit-recognizer)\n",
        "\n",
        "\n",
        "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>\n",
        "\n",
        "\n",
        "---- \n",
        "This code is adopted from the pytorch examples repository. \n",
        "It is licensed under BSD 3-Clause \"New\" or \"Revised\" License.\n",
        "Source: https://github.com/pytorch/examples/\n",
        "LICENSE: https://github.com/pytorch/examples/blob/master/LICENSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0GMJzryJ8WL"
      },
      "source": [
        "# from six.moves import urllib\n",
        "# opener = urllib.request.build_opener()\n",
        "# opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "# urllib.request.install_opener(opener)\n",
        "# ### somehow torch.dataset malfunctioned after an update in March 2021\n",
        "# ### Facebook team issued a hotfix but apparently not loaded in the docker image\n",
        "# ### of Colab yet as of Mar 5, 2021\n",
        "\n",
        "## update as of Mar 11 (as of Mar 28, 2021 this still have to be used)\n",
        "!wget https://sites.wustl.edu/scao/files/2021/03/MNIST.tar_.gz\n",
        "!mv MNIST.tar_.gz MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbHLRTwjJ8WM"
      },
      "source": [
        "## Below is a full pipeline of training that can be found in coding Lecture 8\n",
        "\n",
        "Below is a working baseline for train a neural net.\n",
        "- The first cell is to define everything needed (except the validation part which we will learn to code in next coding HW).\n",
        "- The second cell below is to initialize.\n",
        "- The third cell below is to train the initialized model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-FHXYFjLgcK"
      },
      "source": [
        "# set up the data\n",
        "train = datasets.MNIST(root='./', \n",
        "                       train=True, \n",
        "                       download=True, \n",
        "                       transform = transforms.ToTensor())\n",
        "\n",
        "# set up the loader\n",
        "train_loader = DataLoader(train, batch_size=64)\n",
        "\n",
        "# model\n",
        "class MLP(nn.Module): # subclass of nn.Module \n",
        "    def __init__(self, \n",
        "                 input_size: int,\n",
        "                 output_size: int,\n",
        "                 hidden_size: int = 256):\n",
        "        super(MLP, self).__init__() \n",
        "        self.linear0 = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x): \n",
        "        # getting rid of color channel\n",
        "        x = x.view(x.size(0), -1) \n",
        "        x1 = self.linear0(x)\n",
        "        a1 = self.activation(x1)\n",
        "        output = self.linear1(a1)\n",
        "\n",
        "        return output\n",
        "\n",
        "# optimizer\n",
        "class SGD(Optimizer): # subclass of Optimizer\n",
        "    \"\"\"\n",
        "    Implements the vanilla SGD simplified \n",
        "    from the torch official one for Math 450 WashU\n",
        "    \n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate\n",
        "        \n",
        "    Example:\n",
        "        >>> optimizer = SGD(model.parameters(), lr=1e-2)\n",
        "        >>> optimizer.zero_grad()\n",
        "        >>> loss_fn(model(input), target).backward()\n",
        "        >>> optimizer.step()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, # params: model.parameters()\n",
        "                       lr: float = 1e-3, # input: type = value\n",
        "                       name_input: str = 'SGD'\n",
        "                 ): \n",
        "        # constructor\n",
        "        defaults = dict(lr=lr, name=name_input) \n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None): \n",
        "\n",
        "      for group in self.param_groups:\n",
        "\n",
        "          for param in group['params']:\n",
        "              if param.grad is None:\n",
        "                  continue\n",
        "              grad_param = param.grad.data\n",
        "\n",
        "              param.data -= group['lr']*grad_param\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "def accuracy_score(y_pred, y_true):\n",
        "    '''\n",
        "    A modified acc score from HW 3\n",
        "\n",
        "    Input:\n",
        "        - predicted labels: output from our NN\n",
        "        - true labels: integers from 0 to num_class\n",
        "\n",
        "    Output:\n",
        "        - accuracy score above\n",
        "    '''\n",
        "    y_pred = y_pred.argmax(dim=-1) # convert this to a label\n",
        "    acc = (y_pred == y_true).float().mean()\n",
        "\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nzOkR5oMiww"
      },
      "source": [
        "# get a sample from the train loader\n",
        "sample = next(iter(train_loader))\n",
        " \n",
        "x = sample[0] # data fed into the model\n",
        "y = sample[1] # true label\n",
        "\n",
        "x = x.view(x.size(0), -1) # getting rid of color channel\n",
        "\n",
        "# get input_size\n",
        "input_size = x.size(-1)\n",
        "\n",
        "# get output_size (10 classes)\n",
        "output_size = 10 \n",
        "\n",
        "# hidden layer size\n",
        "hidden_size = 256\n",
        "\n",
        "# initialize our model\n",
        "model = MLP(input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=output_size)\n",
        "\n",
        "# learning rate (step size)\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# equip this model an optimizer\n",
        "optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# choose the loss function\n",
        "# CrossEntropyLoss() in nn avoids taking softmax explicitly\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcB5-mTbNoSn"
      },
      "source": [
        "# train our model for 5 epochs\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    loss_vals_epoch = []\n",
        "    acc_epoch = []\n",
        "\n",
        "    with tqdm(total=len(train_loader)) as pbar:\n",
        "      # use the tqdm with block as a progress bar\n",
        "      for data, targets in train_loader:\n",
        "          \n",
        "        # forward pass\n",
        "        outputs = model(data)\n",
        "\n",
        "        # outputs = torch.log(outputs) if using nn.NLLLoss()\n",
        "        \n",
        "        # loss function\n",
        "        loss = loss_func(outputs, targets)\n",
        "        \n",
        "        # record loss function values\n",
        "        loss_vals_epoch.append(loss.item())\n",
        "        \n",
        "        # clean the gradient from last iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # backprop\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        optimizer.step()\n",
        "        \n",
        "        # check accuracy \n",
        "        # need to detach the variable first (no grad is tracked)\n",
        "        # and move the variable from GPU to CPU memory pool \n",
        "        # if GPU is used\n",
        "        outputs = outputs.detach().cpu()\n",
        "        acc = accuracy_score(outputs, targets)\n",
        "        acc_epoch.append(acc)\n",
        "\n",
        "        # tqdm template\n",
        "        description = f\"epoch:{epoch+1} \"\n",
        "        description += f\"| loss: {np.mean(loss_vals_epoch):.3e}\"\n",
        "        description += f\"| acc: {np.mean(acc_epoch)*100:.2f} %\"\n",
        "        pbar.set_description(description)\n",
        "        pbar.update()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVfOEWlmJ8WM"
      },
      "source": [
        "\n",
        "## Problem 1\n",
        "In the pipeline code above, modified the MLP model to have $k$ ($k\\geq 1$) hidden layers. Notice that you have to modify your code in an automatic way: first change the default input `hidden_size` to a tuple \n",
        "- If the user gives input `hidden_size = (256, )`, then the code is almost unchanged.\n",
        "- If the user gives input `hidden_size = (256, 128)`, then the MLP model should have 2 hidden layers, which have 256 neurons and 128 neurons respectively.\n",
        "- Remember to add a nonlinera activation function in between each hidden layer.\n",
        "\n",
        "There are various way to implement this. We can implement this well within what is taught in class and Math 449. \n",
        "\n",
        "If you have interest to learn things outside of our class's scope, please read the manual below:\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM6PFQ1TY67a"
      },
      "source": [
        "class MLP(nn.Module): # subclass of nn.Module \n",
        "    def __init__(self, \n",
        "                 input_size: int,\n",
        "                 output_size: int,\n",
        "                 hidden_size: tuple = (256, )):\n",
        "        super(MLP, self).__init__() \n",
        "        # modify the code here\n",
        "        self.linear0 = nn.Linear(input_size, hidden_size[0])\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(hidden_size[0], output_size)\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = x.view(x.size(0), -1) \n",
        "        x1 = self.linear0(x)\n",
        "        a1 = self.activation(x1)\n",
        "        output = self.linear1(a1)\n",
        "\n",
        "        # inserting softmax computation here\n",
        "        return output\n",
        "\n",
        "\n",
        "hidden_size = (256, 128, 64)\n",
        "model = MLP(input_size=28*28, output_size=10, hidden_size=hidden_size)\n",
        "for param in model.parameters():\n",
        "  print(param.size())\n",
        "  \n",
        "# expected output:\n",
        "# torch.Size([256, 784])\n",
        "# torch.Size([256])\n",
        "# torch.Size([128, 256])\n",
        "# torch.Size([128])\n",
        "# torch.Size([64, 128])\n",
        "# torch.Size([64])\n",
        "# torch.Size([10, 64])\n",
        "# torch.Size([10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WraWT4RfJ8WN"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "The loss $-\\sum y\\ln \\hat{y}$ is implemented in torch as `nn.NLLLoss` (negative log likelihood loss), which accepts the log of a probability input (i.e., log output of a softmax function). If we replace `nn.CrossEntropyLoss()` with `nn.NLLLoss()`, we have to \n",
        "- Add an explicit softmax computation. \n",
        "- Take the log of the output.\n",
        "\n",
        "Assume we can use the built-in softmax from `nn.functional` or `nn`. Modify the code below to have an explicit softmax function.\n",
        "\n",
        "#### Remark on why we want to do this\n",
        "if we have an imbalanced dataset (like the ones in real life unlike MNIST), which the numbers of samples in each class are drastically different from class to class, using this pipeline is much preferred over applying `nn.CrossEntropyLoss()` directly. Because in this pipeline, it leaves room for adjusting the weight of $-y\\ln \\hat{y}$ computation for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TB4bUJWJ8WN"
      },
      "source": [
        "class MLP(nn.Module): # subclass of nn.Module \n",
        "    def __init__(self, \n",
        "                 input_size: int,\n",
        "                 output_size: int,\n",
        "                 hidden_size: tuple = (256, )):\n",
        "        super(MLP, self).__init__() \n",
        "        self.linear0 = nn.Linear(input_size, hidden_size[0])\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(hidden_size[0], output_size)\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = x.view(x.size(0), -1) \n",
        "        x1 = self.linear0(x)\n",
        "        a1 = self.activation(x1)\n",
        "        output = self.linear1(a1)\n",
        "\n",
        "        # inserting softmax computation here\n",
        "        return output\n",
        "\n",
        "model = MLP(input_size=28*28, output_size=10)\n",
        "sample = next(iter(train_loader))\n",
        "x = sample[0]\n",
        "with torch.no_grad():\n",
        "  yhat = model(x)\n",
        "print(torch.allclose(yhat.sum(axis=1), torch.ones(yhat.size(0)))) \n",
        "# expected output is True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUBYOj2YJ8WO"
      },
      "source": [
        "## Problem 3\n",
        "\n",
        "If the loss function is modified as follows: for $\\epsilon>0$ small $\\epsilon\\ll 1$\n",
        "$$\n",
        "L = -\\sum_{i=1}^{N_{_\\text{batch}}}\n",
        "y_i \\ln \\hat{y}_i + \\epsilon \\|W\\|_2^2,\n",
        "$$\n",
        "where $\\|W\\|_2^2 = \\sum_{l} \\|w_l\\|^2$, where $w_l$ stands for the parameters of $l$-th layer, the $\\ell^2$-norm square is summing up all square of the parameters. When the gradient descent is performed against this loss function, without actually re-implement the loss function, it is equivalent to let each weight decay an extra factor of $(1-\\alpha\\epsilon)$ in each iteration of gradient descent, or simply put, adding an extra $\\alpha \\epsilon W$ to the gradient in each SGD iteration. This is called a \"weight decay\" regularizer.\n",
        "\n",
        "Implement this weight regularizer in the template SGD class below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GjHq_ZzJ8WO"
      },
      "source": [
        "# optimizer\n",
        "class SGD(Optimizer): # subclass of Optimizer\n",
        "    \"\"\"\n",
        "    Implements the vanilla SGD simplified \n",
        "    from the torch official one for Math 450 WashU\n",
        "    \n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate\n",
        "         weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        \n",
        "    Example:\n",
        "        >>> optimizer = SGD(model.parameters(), lr=1e-2)\n",
        "        >>> optimizer.zero_grad()\n",
        "        >>> loss_fn(model(input), target).backward()\n",
        "        >>> optimizer.step()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, # params: model.parameters()\n",
        "                       lr: float = 1e-3, # input: type = value\n",
        "                       weight_decay: float = 0.0,\n",
        "                       name_input: str = 'SGD'\n",
        "                 ): \n",
        "        # constructor\n",
        "        defaults = dict(lr=lr, name=name_input) \n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None): \n",
        "\n",
        "      for group in self.param_groups:\n",
        "          # add weight_decay somewhere here\n",
        "          # notice we can access the weight_decay initialized earlier\n",
        "          # as weight_decay = group['weight_decay']\n",
        "          \n",
        "          for param in group['params']:\n",
        "              if param.grad is None:\n",
        "                  continue\n",
        "              grad_param = param.grad.data\n",
        "\n",
        "              param.data -= group['lr']*grad_param\n",
        "\n",
        "      return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2bMiM0gX3qb"
      },
      "source": [
        "## Problem 4\n",
        "Add the result from Problem 2 and 3 into the pipeline provided in the begining. Compare the result with the baseline. \n",
        "\n",
        "\n",
        "Observing expected and similar result with baseline is a good indication that we can further down improve and change the model, otherwise there must be something wrong with our modification. Changing things bit by bit and then compare with a working baseline is a good habit in machine learning.\n",
        "\n",
        "\n",
        "Expected accuracy: after 5 epochs we should observe similar accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiTEqEm1YEKX"
      },
      "source": [
        "# copy and paste the pipeline here and modify it."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}